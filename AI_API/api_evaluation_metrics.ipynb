{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeKY_gA6fFqU"
      },
      "outputs": [],
      "source": [
        "# evaluation_summary.py\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def calculate_and_summarize_metrics(method_results):\n",
        "    \"\"\"\n",
        "    Calculate and summarize classification evaluation metrics for multiple feature selection methods.\n",
        "\n",
        "    Parameters:\n",
        "        method_results (dict): A dictionary where each key is a method name and its value is a dictionary\n",
        "            containing 'y_true' and 'y_pred' lists or arrays.\n",
        "            Example:\n",
        "            {\n",
        "                \"Method1\": {\"y_true\": [...], \"y_pred\": [...]},\n",
        "                \"Method2\": {\"y_true\": [...], \"y_pred\": [...]},\n",
        "                \"Method3\": {\"y_true\": [...], \"y_pred\": [...]}\n",
        "            }\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple (message, formatted_output, df_table) where:\n",
        "            - message (str): A status message.\n",
        "            - formatted_output (dict): A dictionary with the computed metrics for each method.\n",
        "            - df_table (pd.DataFrame): A DataFrame containing the evaluation metrics for display.\n",
        "    \"\"\"\n",
        "    summary_data = {}\n",
        "\n",
        "    for method, results in method_results.items():\n",
        "        y_true = results.get(\"y_true\")\n",
        "        y_pred = results.get(\"y_pred\")\n",
        "        if y_true is None or y_pred is None:\n",
        "            continue  # Skip if necessary data isn't provided.\n",
        "\n",
        "        # Compute metrics (multiplying by 100 to express percentages)\n",
        "        accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0) * 100\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0) * 100\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0) * 100\n",
        "\n",
        "        summary_data[method] = {\n",
        "            \"Accuracy\": accuracy,\n",
        "            \"Precision\": precision,\n",
        "            \"F-score\": f1,\n",
        "            \"Recall\": recall\n",
        "        }\n",
        "\n",
        "    if not summary_data:\n",
        "        return \"No valid evaluation metrics computed. Please check your input data.\", None, None\n",
        "\n",
        "    # Create a DataFrame for table display\n",
        "    df_table = pd.DataFrame.from_dict(summary_data, orient=\"index\").reset_index()\n",
        "    df_table.rename(columns={\"index\": \"Method\"}, inplace=True)\n",
        "\n",
        "    return \"Done!\", summary_data, df_table\n",
        "\n",
        "def visualize_evaluation_metrics(evaluation_data, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualize classification evaluation metrics as a grouped bar chart.\n",
        "\n",
        "    Parameters:\n",
        "        evaluation_data (dict): A dictionary where keys are method names and values are dictionaries\n",
        "            containing classification metrics.\n",
        "            Example:\n",
        "            {\n",
        "                \"Method1\": {\"Accuracy\": 93.3, \"Precision\": 95.4, \"F-score\": 88.9, \"Recall\": 61.2},\n",
        "                \"Method2\": {\"Accuracy\": 93.0, \"Precision\": 99.9, \"F-score\": 24.7, \"Recall\": 14.1},\n",
        "                \"Method3\": {\"Accuracy\": 93.0, \"Precision\": 99.9, \"F-score\": 19.1, \"Recall\": 10.6}\n",
        "            }\n",
        "        save_path (str): Optional path to save the generated plot image.\n",
        "\n",
        "    Returns:\n",
        "        fig: The matplotlib figure object.\n",
        "    \"\"\"\n",
        "    # Extract metric names from one of the methods.\n",
        "    metrics_keys = list(next(iter(evaluation_data.values())).keys())\n",
        "    methods = list(evaluation_data.keys())\n",
        "    n_metrics = len(metrics_keys)\n",
        "    n_methods = len(methods)\n",
        "\n",
        "    # Prepare a 2D array of data (rows: metrics, columns: methods)\n",
        "    data = np.zeros((n_metrics, n_methods))\n",
        "    for i, metric in enumerate(metrics_keys):\n",
        "        for j, method in enumerate(methods):\n",
        "            data[i, j] = evaluation_data[method].get(metric, 0)\n",
        "\n",
        "    # Create a grouped bar chart.\n",
        "    x = np.arange(n_metrics)  # positions for each metric on the x-axis\n",
        "    width = 0.8 / n_methods   # width of each bar\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    for j, method in enumerate(methods):\n",
        "        ax.bar(x + j * width, data[:, j], width, label=method)\n",
        "\n",
        "    ax.set_ylabel('Metric Value (%)')\n",
        "    ax.set_title('Classification Evaluation Metrics by Method')\n",
        "    ax.set_xticks(x + width * (n_methods - 1) / 2)\n",
        "    ax.set_xticklabels(metrics_keys)\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    return fig"
      ]
    }
  ]
}